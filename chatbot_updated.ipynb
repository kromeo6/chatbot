{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOp04wQNT3l+UrTWMLJ6NEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kromeo6/chatbot/blob/main/chatbot_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r68xEqjWOYB"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8f3LRxo2si5",
        "outputId": "128c8f49-e83e-4638-ff3b-21544ddb8deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -V"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "905a8fuiWYTZ",
        "outputId": "de3ff800-00b6-4667-ea18-7381b90115ce",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-605df322-98ca-4cce-84b9-10393d13ec8d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-605df322-98ca-4cce-84b9-10393d13ec8d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving intents1.json to intents1 (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lhGv6aWi9ek"
      },
      "source": [
        "import io\n",
        "data = pd.read_json(io.StringIO(uploaded['intents1.json'].decode('utf-8')))   #reading json file"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dABhoZcU3gEA"
      },
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pxnSBwIjg90",
        "outputId": "36262b72-3027-426e-c889-4d8a225cc110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import json\n",
        "#import tensorflow\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQH-8i3o2rk"
      },
      "source": [
        "geo_words_mapping = {'ასაკი':['ასაკმა','ასაკს','ასაკის','ასასაკით','ასაკად', 'ასაკში', 'ასაკთან', 'ასაკზე'],\n",
        " 'გამარჯობა':['გამარჯობათ','გამარჯობად','გაგიმარჯოს','მოგესალმებით','სალამი','პრივეტ','ჩაო'],\n",
        " 'გქვია':['გქვიათ','დაგარქვა','დაგარქვათ','შეგარქვათ'],\n",
        " 'დაგიძახო':['დაგიძახოთ','დაძახება','გეძახდით','გეძახდათ','გეძახდა'],\n",
        " 'კარგად':['კარგათ', 'კარგი'],\n",
        " 'მადლობა':['მადლობთ','გმადლობთ','მადლობელი', 'მადლიერი','მადლობ'],\n",
        " 'მუშაობა':['მუშაობთ','მუშაობ','სამუშაო','სამუშავო','იმუშავებთ','მუშაობდით','მუშაობდი'],\n",
        " 'ნახვამდის':['ნახვამდის'],\n",
        " 'რამდენი':['რამდენში', 'რამდენად','რამდენზე','რამდენით'],\n",
        " 'როგორ':['რანაირად','რანაირათ'],\n",
        " 'რომელი':['რომელ','რომელიმე','რომელმა','რომელს','რომელის','რომელით'],\n",
        " 'საათი':['საატი','საათამდე','საატამდე','საათზე','საატზე','საათში','სათამდე','საათით','საათები'],\n",
        " 'სახელი':['სახელად','სახელათ','სახელში','სახელმა','სახელს','სახელით','სახელთან'],\n",
        " 'წელი':['წლის','წელმა','წელს','წლით','წლად','წელო','წელში','წელთან'],\n",
        " 'ხარ':['ხართ']\n",
        " }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpHkBESkyFP2"
      },
      "source": [
        "def replace_words(l, d):  \n",
        "  p = []\n",
        "  for w in l:\n",
        "    lnth = len(p)\n",
        "    for k in d:\n",
        "      if w in d[k]:\n",
        "        p.append(k)\n",
        "        continue\n",
        "    if len(p) == lnth:\n",
        "      p.append(w)   \n",
        "  return p    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO0cZtprjhxG"
      },
      "source": [
        "def get_training_data_from_json (data, d=0, replace=True):\n",
        "        \n",
        "    words = [] #we need to put all the used word in patterns into this list\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern) #dissolve sentance into words            \n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds) #start to make training data\n",
        "            docs_y.append(intent[\"tag\"]) #start to make the target\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"]) #labels are set of tags\n",
        "    \n",
        "    words = sorted(list(set(words)))  # set function gives us the unique words\n",
        "    labels = sorted(labels)\n",
        "    if replace:\n",
        "      docs_x_replaced = []\n",
        "      words = replace_words(words, d)\n",
        "      for doc in docs_x:\n",
        "        docs_x_replaced.append(replace_words(doc ,d))\n",
        "      return docs_x_replaced, docs_y, words, labels\n",
        "    return docs_x, docs_y, words, labels  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ppb8wzV-Evf"
      },
      "source": [
        "def to_vectors(docs_x,docs_y, words,labels):\n",
        "  training = []\n",
        "  output = []\n",
        "  out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "  # we need to convert word into vectors. we created list of all unique words into \"words\" list.\n",
        "\n",
        "  for x, doc in enumerate(docs_x):\n",
        "      bag = []\n",
        "\n",
        "      #wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "      wrds = [w for w in doc]  # es sworia mara zedmetia\n",
        "\n",
        "      for w in words:\n",
        "          if w in wrds:\n",
        "              bag.append(1)\n",
        "          else:\n",
        "              bag.append(0)\n",
        "\n",
        "      output_row = out_empty[:]\n",
        "      output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "      training.append(bag)\n",
        "      output.append(output_row)\n",
        "      training = np.array(training)\n",
        "      output = np.array(output)\n",
        "      return training, output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FKpWsfUaV0F"
      },
      "source": [
        "docs_x, docs_y, words, labels = get_training_data_from_json (data, d=geo_words_mapping)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoPMj_biSFw7"
      },
      "source": [
        "training, output = to_vectors(docs_x,docs_y, words,labels)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_a8H-__tF32"
      },
      "source": [
        "#words = [stemmer.stem(w.lower()) for w in words if w != \"?\"] #all the letters should be NOT capital"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXOl1i9fgx3X"
      },
      "source": [
        "# my_dict = json.dumps(d, ensure_ascii=False)\n",
        "# print(my_dict)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0pNDr_FQczX",
        "outputId": "ad9e10d4-8b97-44ce-aa78-70dfc0ee499e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBnhYhOWmZyz",
        "outputId": "9ce71112-5693-4230-a690-c0a089294380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tflearn\n",
        "import tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPzQwkTcOAFE",
        "outputId": "8f8bf685-8762-4671-aada-21a6e2b128de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tensorflow.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/layers/core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/layers/core.py:145: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/objectives.py:70: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/layers/estimator.py:189: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:571: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:164: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:165: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:166: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:167: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEPfF0OWOybP",
        "outputId": "a16f96cb-6905-4cda-f835-b251d29c0f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(training, output, n_epoch=100, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: HMB4P7\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
            "---------------------------------\n",
            "Training samples: 1\n",
            "Validation samples: 0\n",
            "--\n",
            "Training Step: 1  | time: 0.321s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m1.44851\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 002 | loss: 1.44851 - acc: 0.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.57878\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 003 | loss: 1.57878 - acc: 0.8182 -- iter: 1/1\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.59919\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 004 | loss: 1.59919 - acc: 0.9545 -- iter: 1/1\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.60268\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 005 | loss: 1.60268 - acc: 0.9860 -- iter: 1/1\n",
            "--\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.60253\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 006 | loss: 1.60253 - acc: 0.9950 -- iter: 1/1\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.60138\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 007 | loss: 1.60138 - acc: 0.9980 -- iter: 1/1\n",
            "--\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.59989\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 008 | loss: 1.59989 - acc: 0.9991 -- iter: 1/1\n",
            "--\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.59826\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 009 | loss: 1.59826 - acc: 0.9996 -- iter: 1/1\n",
            "--\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.59655\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 010 | loss: 1.59655 - acc: 0.9998 -- iter: 1/1\n",
            "--\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.59477\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 011 | loss: 1.59477 - acc: 0.9999 -- iter: 1/1\n",
            "--\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.59294\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 012 | loss: 1.59294 - acc: 0.9999 -- iter: 1/1\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.59105\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 013 | loss: 1.59105 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.58910\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 014 | loss: 1.58910 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.58709\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 015 | loss: 1.58709 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.58500\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 016 | loss: 1.58500 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.58285\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 017 | loss: 1.58285 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.58061\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 018 | loss: 1.58061 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.57829\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 019 | loss: 1.57829 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.57588\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 020 | loss: 1.57588 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.57336\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 021 | loss: 1.57336 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.57075\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 022 | loss: 1.57075 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.56802\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 023 | loss: 1.56802 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.56517\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 024 | loss: 1.56517 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.56219\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 025 | loss: 1.56219 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.55908\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 026 | loss: 1.55908 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.55583\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 027 | loss: 1.55583 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.55242\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 028 | loss: 1.55242 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.54885\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 029 | loss: 1.54885 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.54510\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 030 | loss: 1.54510 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.54118\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 031 | loss: 1.54118 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.53707\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 032 | loss: 1.53707 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.53276\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 033 | loss: 1.53276 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.52823\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 034 | loss: 1.52823 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.52349\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 035 | loss: 1.52349 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.51851\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 036 | loss: 1.51851 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.51329\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 037 | loss: 1.51329 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.50781\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 038 | loss: 1.50781 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.50207\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 039 | loss: 1.50207 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.49606\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 040 | loss: 1.49606 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.48975\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 041 | loss: 1.48975 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.48315\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 042 | loss: 1.48315 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.47623\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 043 | loss: 1.47623 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.46899\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 044 | loss: 1.46899 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.46142\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 045 | loss: 1.46142 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.45350\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 046 | loss: 1.45350 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.44522\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 047 | loss: 1.44522 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.43658\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 048 | loss: 1.43658 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.42755\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 049 | loss: 1.42755 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.41813\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 050 | loss: 1.41813 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.40831\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 051 | loss: 1.40831 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.39808\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 052 | loss: 1.39808 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.38742\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 053 | loss: 1.38742 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.37633\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 054 | loss: 1.37633 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.36480\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 055 | loss: 1.36480 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.35282\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 056 | loss: 1.35282 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.34038\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 057 | loss: 1.34038 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.32747\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 058 | loss: 1.32747 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.31409\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 059 | loss: 1.31409 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.30024\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 060 | loss: 1.30024 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.28590\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 061 | loss: 1.28590 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.27107\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 062 | loss: 1.27107 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.25576\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 063 | loss: 1.25576 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.23995\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 064 | loss: 1.23995 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.22366\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 065 | loss: 1.22366 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.20688\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 066 | loss: 1.20688 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.18961\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 067 | loss: 1.18961 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.17186\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 068 | loss: 1.17186 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.15363\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 069 | loss: 1.15363 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.13493\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 070 | loss: 1.13493 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.11578\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 071 | loss: 1.11578 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.09619\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 072 | loss: 1.09619 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.07616\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 073 | loss: 1.07616 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.05572\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 074 | loss: 1.05572 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.03488\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 075 | loss: 1.03488 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.01366\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 076 | loss: 1.01366 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.99209\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 077 | loss: 0.99209 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.97020\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 078 | loss: 0.97020 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.94800\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 079 | loss: 0.94800 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.92554\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 080 | loss: 0.92554 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.90283\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 081 | loss: 0.90283 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.87993\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 082 | loss: 0.87993 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.85660\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 083 | loss: 0.85660 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.80892\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 084 | loss: 0.80892 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.80892\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 085 | loss: 0.80892 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.78469\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 086 | loss: 0.78469 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.76029\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 087 | loss: 0.76029 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.73577\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 088 | loss: 0.73577 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.71120\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 089 | loss: 0.71120 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.68665\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 090 | loss: 0.68665 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.63783\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 091 | loss: 0.63783 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.63783\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 092 | loss: 0.63783 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.61368\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 093 | loss: 0.61368 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.58978\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 094 | loss: 0.58978 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.56619\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 095 | loss: 0.56619 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.54296\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 096 | loss: 0.54296 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.52014\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 097 | loss: 0.52014 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.49777\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 098 | loss: 0.49777 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.47590\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 099 | loss: 0.47590 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.45456\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 100 | loss: 0.45456 - acc: 1.0000 -- iter: 1/1\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}